# Alarm 1 
name: testrun-2024-02-27-22-49/trial-00-0004/0001
## What happened
Pod test-cluster-tikv-0 crashed.
tidb-scheduler fail to do the Leaderelection or restart[3].

## Root Cause
### tidb-scheduler fail Leaderelection and lost
```
"leaderelection.go:250 attempting to acquire leader lease acto-namespace/tidb-scheduler...",
"leaderelection.go:260 successfully acquired lease acto-namespace/tidb-scheduler",
"leaderelection.go:332 error retrieving resource lock acto-namespace/tidb-scheduler: Get \"https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/acto-namespace/leases/tidb-scheduler?timeout=5s\": net/http: request canceled (Client.Timeout exceeded while awaiting headers)",
"leaderelection.go:369 Failed to update lock: client rate limiter Wait returned an error: context deadline exceeded",
"leaderelection.go:285 failed to renew lease acto-namespace/tidb-scheduler: timed out waiting for the condition",
"server.go:242 \"Leaderelection lost\""
```
There are some issue about similar problem 
1. https://github.com/kubernetes/kubernetes/issues/96923 
2. https://github.com/elastic/beats/issues/34998 
3. https://github.com/kubernetes/kubernetes/issues/108380
4. https://github.com/aws/karpenter-provider-aws/issues/1283

### test-cluster-tikv-0 crashed
```
[ERROR] [util.rs:462] [\"request failed, retry\"] [err_code=KV:Pd:Grpc] [err=\"Grpc(RpcFailure(RpcStatus { code: 4-DEADLINE_EXCEEDED, message: \\\"Deadline Exceeded\\\", details: [] }))\"]"
```
There are some issue about similar problem  
1. https://github.com/tikv/tikv/issues/15184

## Expected behavior?
test-cluster-tikv-0 not crashed, I think it may caused by Leaderelection lost from tidb-schduler. From the issues listed, it could solved by add
```
    - --leader-elect=true
    - --leader-elect-lease-duration=60s
    - --leader-elect-renew-deadline=40s
``` 
in the file tidb-schduler config file, which extend the time constrain for lease and renew.


# Alarm 2-8 
name: 
1. testrun-2024-02-27-22-49/trial-01-0003/0001
2. testrun-2024-02-27-22-49/trial-01-0004/0002
3. testrun-2024-02-27-22-49/trial-01-0005/0001
4. testrun-2024-02-27-22-49/trial-01-0007/0001
5. testrun-2024-02-27-22-49/trial-01-0008/0002
6. testrun-2024-02-27-22-49/trial-01-0009/0001
7. testrun-2024-02-27-22-49/trial-01-0010/0002
## What happened
false alarm

"message": "create Pod test-cluster-tikv-0 in StatefulSet test-cluster-tikv failed error: Pod \"test-cluster-tikv-0\" is invalid: spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[0].namespace: Invalid value: \"ACTOKEY\": a lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?')",

## Root Cause
"ACTOKEY" is invalid for spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution.namespace
there is a similar issue:
1. https://github.com/kubernetes/kubernetes/issues/94088

## Expected behavior?
Should change the "ACTOKEY" to something valid

# Alarm 9 
name: testrun-2024-02-27-22-49/trial-01-0011/0005

## What happened
Tidb schduler and Tidb controler manager leader election lost.
```
F0228 07:30:43.370448       1 main.go:214 leader election lost",
```
## Root Cause
Same as Alarm 1.
But by analysis the delta between Alarm 1 and 9, they both do 
```
"root['spec']['tikv']['additionalContainers'][0][name]": {
        "prev": "NotPresent",
        "curr": "tkzmudrbew",
        "path": {
            "path": [
                    "spec",
                    "tikv",
                    "additionalContainers",
                    0,
                    "name"
            ]
        }
},
```
And it say in the doc, 
```
If the container names in this field match with the ones generated by TiDB Operator, the container configurations will be merged into the containers generated by TiDB Operator via strategic merge patch.
```
Maybe the additional container have the same name and merge with the containers generated by TiDB Operator. 
## Expected behavior?
Same as Alarm 1.

# Alarm 10 
name: testrun-2024-02-27-22-49/trial-01-0046/0005

## What happened
false alarm

Acto change the podSecurityContext to run AsrunAsNonRoot, Than raise the Error.
```
"path": [
        "spec",
        "tikv",
        "podSecurityContext",
        "runAsNonRoot"
]
```
```
"message": "Error: container has runAsNonRoot and image will run as root (pod: \"test-cluster-tikv-0_acto-namespace(83ecafd1-d395-4ccd-8f35-e69d59c524a5)\", container: tikv)"
```
## Root Cause
I find the root code here but not find any insights.
```
if baseTiKVSpec.PodSecurityContext() != nil && len(baseTiKVSpec.PodSecurityContext().Sysctls) > 0 {
    for _, sysctl := range baseTiKVSpec.PodSecurityContext().Sysctls {
        sysctls = sysctls + fmt.Sprintf(" %s=%s", sysctl.Name, sysctl.Value)
    }
    privileged := true
    initContainers = append(initContainers, corev1.Container{
        Name:  "init",
        Image: tc.HelperImage(),
        Command: []string{
            "sh",
            "-c",
            sysctls,
        },
        SecurityContext: &corev1.SecurityContext{
            Privileged: &privileged,
        },
        // Init container resourceRequirements should be equal to app container.
        // Scheduling is done based on effective requests/limits,
        // which means init containers can reserve resources for
        // initialization that are not used during the life of the Pod.
        // ref:https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#resources
        Resources: controller.ContainerResource(tc.Spec.TiKV.ResourceRequirements),
    })
}
```

## Expected behavior?
But the acto set the podSecurityContext:runAsNonRoot as true.
```
runAsNonRoot: true
```
But in all example, the setting for runAsNonRoot should be user ID:
```
podSecurityContext:
    runAsUser: 1000
    runAsGroup: 2000
    fsGroup: 2000
```
Maybe acto need to identical that and set the user ID.




